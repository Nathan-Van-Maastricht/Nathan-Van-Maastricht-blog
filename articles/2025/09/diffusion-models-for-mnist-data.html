
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />

  <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="https://nathan-van-maastricht.github.io/Nathan-Van-Maastricht-blog/theme/stylesheet/style.min.css">


    <link id="pygments-light-theme" rel="stylesheet" type="text/css"
          href="https://nathan-van-maastricht.github.io/Nathan-Van-Maastricht-blog/theme/pygments/github.min.css">



  <link rel="stylesheet" type="text/css" href="https://nathan-van-maastricht.github.io/Nathan-Van-Maastricht-blog/theme/font-awesome/css/fontawesome.css">
  <link rel="stylesheet" type="text/css" href="https://nathan-van-maastricht.github.io/Nathan-Van-Maastricht-blog/theme/font-awesome/css/brands.css">
  <link rel="stylesheet" type="text/css" href="https://nathan-van-maastricht.github.io/Nathan-Van-Maastricht-blog/theme/font-awesome/css/solid.css">






<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-S4XSZFZ0ZK"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-S4XSZFZ0ZK');
</script>






 

<meta name="author" content="Nathan Van Maastricht" />
<meta name="description" content="Demo Before I even get started on what I&#39;ve done, here is a demo that you can play with. Introduction After playing around with reconstruction and classification of MNIST digits last week and talking to a few colleagues about the project, I decided to investigate diffusion models. I broadly knew …" />
<meta name="keywords" content="machine learning, mnist">


  <meta property="og:site_name" content="Nathan Van Maastricht Blog"/>
  <meta property="og:title" content="Diffusion Models for MNIST Data"/>
  <meta property="og:description" content="Demo Before I even get started on what I&#39;ve done, here is a demo that you can play with. Introduction After playing around with reconstruction and classification of MNIST digits last week and talking to a few colleagues about the project, I decided to investigate diffusion models. I broadly knew …"/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:url" content="https://nathan-van-maastricht.github.io/Nathan-Van-Maastricht-blog/articles/2025/09/diffusion-models-for-mnist-data.html"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2025-09-27 18:22:00+09:30"/>
  <meta property="article:modified_time" content=""/>
  <meta property="article:author" content="https://nathan-van-maastricht.github.io/Nathan-Van-Maastricht-blog/author/nathan-van-maastricht.html">
  <meta property="article:section" content="Machine Learning"/>
  <meta property="article:tag" content="machine learning"/>
  <meta property="article:tag" content="mnist"/>
  <meta property="og:image" content="">

  <title>Nathan Van Maastricht Blog &ndash; Diffusion Models for MNIST Data</title>


</head>
<body class="light-theme">

<aside>
  <div>
    <a href="https://nathan-van-maastricht.github.io/Nathan-Van-Maastricht-blog/">
      <img src="https://nathan-van-maastricht.github.io/Nathan-Van-Maastricht-blog/theme/img/profile.png" alt="" title="">
    </a>

    <h1>
      <a href="https://nathan-van-maastricht.github.io/Nathan-Van-Maastricht-blog/"></a>
    </h1>



    <nav>
      <ul class="list">


            <li>
              <a target="_self"
                 href="https://nathan-van-maastricht.github.io/Nathan-Van-Maastricht-blog/pages/about.html#about">
                About
              </a>
            </li>

          <li>
            <a target="_self" href="https://www.linkedin.com/in/nathan-van-maastricht-72a177324/" >LinkedIn</a>
          </li>
          <li>
            <a target="_self" href="https://github.com/Nathan-Van-Maastricht" >Github</a>
          </li>
      </ul>
    </nav>

  </div>

</aside>
  <main>


<article class="single">
  <header>
      
    <h1 id="diffusion-models-for-mnist-data">Diffusion Models for MNIST Data</h1>
    <p>
      Posted on Sat 27 September 2025 in <a href="https://nathan-van-maastricht.github.io/Nathan-Van-Maastricht-blog/category/machine-learning.html">Machine Learning</a>

    </p>
    <p>
      Read time: 12 minutes
    </p>
  </header>


  <div>
    <h1>Demo</h1>
<p><a href="https://nathan-van-maastricht.github.io/diffusion_demo/">Before I even get started on what I've done, here is a demo that you can play with.</a></p>
<h1>Introduction</h1>
<p>After playing around with <a href="learning-to-reconstruct-and-classify-mnist-digits.html">reconstruction and classification of MNIST digits last week</a> and talking to a few colleagues about the project, I decided to investigate diffusion models. I broadly knew what was happening within in them, they were removing noise from an image, slowly revealing the image that is actually desired. I didn't realise how close I was to already having a diffusion model though.</p>
<p>The goal of a diffusion model is to learn a process for a given dataset that can generate new elements that have a similar distribution to the original dataset. There are at least two other types of models that try to solve the problem of generating new elements in a dataset, one is GANs and the other is Normalising Flows. I'm not going to talk about GANs or Normalising Flows because I haven't actually implemented one, although I have taken inspiration from GANs in RL applications, which I might write about one day.</p>
<h1>Data</h1>
<p>Similar to last week, I will be using the MNIST Digit dataset, but I have also added on the MNIST Fashion dataset. A small update on the reconstruction and classifying task, I did try it with the Fashion dataset, and it required a lot of tweaking, but I got quite good performance on that dataset too.</p>
<p>For this week though, I didn't do any preprocessing to attempt to improve performance, not even a translation of the image. I have kept the two datasets separate too, training two different models, one to handle the digits, one to handle the fashion images. Seeing as both datasets have the same number of classes though and image sizes, I just had to change the dataloader appropriately, all other code could remain the same.</p>
<p>I did do one preprocessing step that I don't believe affected the outcome - although I didn't do an ablation study - I added 2 pixels of black padding around the border to bring each image up to <span class="math">\(32\times32\)</span> pixels. My reasoning for this is the CIFAR-10 dataset is <span class="math">\(32\times32\)</span> pixels, and this modification will make it relatively easy to train on the CIFAR-10 dataset.</p>
<h1>Problem</h1>
<p>So what is the actual problem I'm attempting to solve. Mostly I just want to learn about diffusion models. The approach we're taking to that problem is to generate images that are <span class="math">\(32\times32\)</span> of Gaussian noise, and then reconstruct a digit that a person would recognise as belonging to the digit or fashion datasets.</p>
<p>To give an idea of how hard this problem seems on the surface, this is a picture of random noise</p>
<p><img alt="Gaussian noise" src="../../../images/diffusion_mnist/noise.png"></p>
<p>and through a bunch of transforms on this image we are trying to end up with something like this</p>
<p><img alt="Goal example" src="../../../images/diffusion_mnist/goal.png"></p>
<h1>Model</h1>
<p>The model is technically of the form a Denoising Diffusion Probabilistic Model (DDPM) which was originally introduced in 2015. The implementation I have gone for is to keep a separate scheduler to manage the noise levels, and a U-Net to predict the noise.</p>
<h2>DDPM Scheduler</h2>
<p>There are three core parameters, and a few derived parameters from those core parameters. They are used both in the training process and inference process.</p>
<ol>
<li><span class="math">\(\beta\)</span> is a linearly increasing sequence of variances, and I have relatively arbitrarily picked the lower bound to be 0.0001, and the upper bound to be 0.02. It is increased across all the time steps. This variable controls how much noise is added at each time step of the forward process.</li>
<li><span class="math">\(\alpha\)</span> is derived from <span class="math">\(\beta\)</span>'s, and is simply <span class="math">\(1-\beta\)</span></li>
<li><span class="math">\(\bar\alpha\)</span> is the cumulative product of the <span class="math">\(\alpha\)</span> values, that is <div class="math">$$\bar\alpha_t=\prod_{i=1}^t\alpha_i.$$</div> This is another key component used to determine the total noise applied up to time step <span class="math">\(t\)</span> in the diffusion process.</li>
</ol>
<p>I then precompute values such as <span class="math">\(\sqrt{1-\bar\alpha_t}\)</span> and <span class="math">\(\sqrt{1/\alpha_t}\)</span> which are used in both the forward process and the denoising process.</p>
<h2>U-Net</h2>
<p>The U-Net model is responsible for predicting the noise that was added to a noisy image at a given time step <span class="math">\(t\)</span> and for a specific class label.</p>
<h3>Time Embedding</h3>
<p>There is a time embedding that takes the time step <span class="math">\(t\)</span> and converts it into a high dimension vector using a sinusoidal positional encoding, and then uses a SiLU as an activation function. This embedding allows the model to determine where in the diffusion process it is. The time embedding is in a space with 256 dimensions.</p>
<h3>Label Embedding</h3>
<p>I'm conditioning the model on a label, which in the case of MNIST digits or fashion is just an integer between 0 and 9. This is also partially why I haven't made a model combining both datasets, it's pure laziness not writing a function to transform the label for one of them. The actual embedding portion is quite small, just two layers with a SiLU activation. The label is embedded into a space with 256 dimensions.</p>
<h3>Combined Embedding</h3>
<p>I then combine the time and label embedding by simply summing them. It is crucial that the two spaces have the same dimension because of this, and the space is large enough to be representative. 256 dimensions is probably overkill for this problem, but I have not experimented with making the space smaller yet.</p>
<h3>Resnet Block</h3>
<p>The fundamental building block of the network is a resnet block. It has two convolutional layers with group normalisation and SiLU activation. It incorporates a residual connection within the block, in some cases it is a convulation layer, in other cases it is just the identity.</p>
<h3>Downsampling Path</h3>
<p>The downsampling starts with a single convulation layer to increase the number of channels, then there are three resnet blocks with a max pool layer after each block to increase the number of channels and decrease the spatial dimensions.</p>
<h3>Bottleneck</h3>
<p>A resnet block to make sure the downsampling path and upsampling path match.</p>
<h3>Upsampling Path</h3>
<p>The path has three resnet blocks, with an upsample at the beginning of each block. There are skip connections from the corresponding level in the downsampling path through concatenation.</p>
<h3>Output</h3>
<p>There is a final convolutional layer that is used to map back to the original 1 channel, which is then interpreted as the predicted noise.</p>
<h3>Layers</h3>
<p>The following table shows how the parameters are distributed throughout the model. Hopefully the names are able to capture where in the model they are.</p>
<div class="highlight"><pre><span></span><code>+-------------------------------+------------+
|            Modules            | Parameters |
+-------------------------------+------------+
| time_embedding.linear1.weight |   65536    |
|  time_embedding.linear1.bias  |    256     |
| time_embedding.linear2.weight |   65536    |
|  time_embedding.linear2.bias  |    256     |
|    label_embedding.0.weight   |    2560    |
|    label_embedding.2.weight   |   65536    |
|     label_embedding.2.bias    |    256     |
|        init_conv.weight       |    288     |
|         init_conv.bias        |     32     |
|       down1.conv1.weight      |    9216    |
|        down1.conv1.bias       |     32     |
|    down1.groupnorm1.weight    |     32     |
|     down1.groupnorm1.bias     |     32     |
|     down1.emb_proj.weight     |    8192    |
|      down1.emb_proj.bias      |     32     |
|       down1.conv2.weight      |    9216    |
|        down1.conv2.bias       |     32     |
|    down1.groupnorm2.weight    |     32     |
|     down1.groupnorm2.bias     |     32     |
|       down2.conv1.weight      |   18432    |
|        down2.conv1.bias       |     64     |
|    down2.groupnorm1.weight    |     64     |
|     down2.groupnorm1.bias     |     64     |
|     down2.emb_proj.weight     |   16384    |
|      down2.emb_proj.bias      |     64     |
|       down2.conv2.weight      |   36864    |
|        down2.conv2.bias       |     64     |
|    down2.groupnorm2.weight    |     64     |
|     down2.groupnorm2.bias     |     64     |
|     down2.skip_conv.weight    |    2048    |
|      down2.skip_conv.bias     |     64     |
|       down3.conv1.weight      |   73728    |
|        down3.conv1.bias       |    128     |
|    down3.groupnorm1.weight    |    128     |
|     down3.groupnorm1.bias     |    128     |
|     down3.emb_proj.weight     |   32768    |
|      down3.emb_proj.bias      |    128     |
|       down3.conv2.weight      |   147456   |
|        down3.conv2.bias       |    128     |
|    down3.groupnorm2.weight    |    128     |
|     down3.groupnorm2.bias     |    128     |
|     down3.skip_conv.weight    |    8192    |
|      down3.skip_conv.bias     |    128     |
|    bottleneck.conv1.weight    |   294912   |
|     bottleneck.conv1.bias     |    256     |
|  bottleneck.groupnorm1.weight |    256     |
|   bottleneck.groupnorm1.bias  |    256     |
|   bottleneck.emb_proj.weight  |   65536    |
|    bottleneck.emb_proj.bias   |    256     |
|    bottleneck.conv2.weight    |   589824   |
|     bottleneck.conv2.bias     |    256     |
|  bottleneck.groupnorm2.weight |    256     |
|   bottleneck.groupnorm2.bias  |    256     |
|  bottleneck.skip_conv.weight  |   32768    |
|   bottleneck.skip_conv.bias   |    256     |
|        up1.conv1.weight       |   442368   |
|         up1.conv1.bias        |    128     |
|     up1.groupnorm1.weight     |    128     |
|      up1.groupnorm1.bias      |    128     |
|      up1.emb_proj.weight      |   32768    |
|       up1.emb_proj.bias       |    128     |
|        up1.conv2.weight       |   147456   |
|         up1.conv2.bias        |    128     |
|     up1.groupnorm2.weight     |    128     |
|      up1.groupnorm2.bias      |    128     |
|      up1.skip_conv.weight     |   49152    |
|       up1.skip_conv.bias      |    128     |
|        up2.conv1.weight       |   110592   |
|         up2.conv1.bias        |     64     |
|     up2.groupnorm1.weight     |     64     |
|      up2.groupnorm1.bias      |     64     |
|      up2.emb_proj.weight      |   16384    |
|       up2.emb_proj.bias       |     64     |
|        up2.conv2.weight       |   36864    |
|         up2.conv2.bias        |     64     |
|     up2.groupnorm2.weight     |     64     |
|      up2.groupnorm2.bias      |     64     |
|      up2.skip_conv.weight     |   12288    |
|       up2.skip_conv.bias      |     64     |
|        up3.conv1.weight       |   27648    |
|         up3.conv1.bias        |     32     |
|     up3.groupnorm1.weight     |     32     |
|      up3.groupnorm1.bias      |     32     |
|      up3.emb_proj.weight      |    8192    |
|       up3.emb_proj.bias       |     32     |
|        up3.conv2.weight       |    9216    |
|         up3.conv2.bias        |     32     |
|     up3.groupnorm2.weight     |     32     |
|      up3.groupnorm2.bias      |     32     |
|      up3.skip_conv.weight     |    3072    |
|       up3.skip_conv.bias      |     32     |
|       output_conv.weight      |    288     |
|        output_conv.bias       |     1      |
+-------------------------------+------------+
Total Trainable Params: 2447681
</code></pre></div>

<h1>Training</h1>
<p>The main idea of the training is to get the model to predict the noise that was added to an image at a randomly chosen time step.</p>
<p>First a random number is generated to represent the time step, between 0 and the maximum time step.</p>
<p>Then random noise is generated.</p>
<p>Using the <span class="math">\(\bar\alpha\)</span> values we can calculate the noisy image by </p>
<div class="math">$$\sqrt{\bar{\alpha}}\times C+\sqrt{1-\bar\alpha}\times N$$</div>
<p> where <span class="math">\(C\)</span> is the clean image, and <span class="math">\(N\)</span> is the noise.</p>
<p>This is is often called the "reparametrerisation trick" to make sure the sampling process is actually differentiable.</p>
<p>This noisy image, which time step, and the label are all passed to the model where it makes a prediction on the noise.</p>
<h2>Loss Function</h2>
<p>The loss function is a simple mean squared error and is calculated between the predicted noise and the actual noise that was generated.</p>
<h2>Hyperparameters</h2>
<p>Similar to my last post, I didn't do too much in the way of tweaking hyperparameters to make performance better. It worked with values I thought were relatively sensible far better than I would have expected.</p>
<ul>
<li>Learning rate: <span class="math">\(5\times10^{-5}\)</span></li>
<li>Number of time steps: 1000</li>
<li>Batch size: 64</li>
<li>Optimiser: AdamW with pytorch default parameters other than learning rate</li>
</ul>
<h1>Inference</h1>
<p>The image generation phase starts with pure noise and runs the diffusion process in reverse, eventually getting to a clean image.</p>
<p>The process is an iterative Markov chain that starts at time step <span class="math">\(t=T\)</span> and ends at time step <span class="math">\(t=0\)</span>.</p>
<p>We start with pure noise and keep track of the current image, the current timestamp, and the label the model is trying to produce an image for.</p>
<p>Within each iteration, the model makes a best prediction for the mean of the noise, and then a new mean is calculated from this prediction by </p>
<div class="math">$$\frac{1}{\sqrt{\alpha_t}}\times\left(I - \left(\frac{\beta_t}{\sqrt{1-\bar\alpha_t}}\right)\times N\right)$$</div>
<p>where <span class="math">\(I\)</span> is the original image at this time step, and <span class="math">\(N\)</span> is the predicted noise for <span class="math">\(I\)</span>.</p>
<p>From this we get the image that is being fed into the next iteration by taking the precomputed posterior variance and scaling a Gaussian distribution by this variance, then adding it to the mean calculated in the previous step.</p>
<p>If we have stepped back to time <span class="math">\(t=0\)</span>, then this new image is our output, otherwise we repeat the process again.</p>
<h1>Results</h1>
<p>I didn't actually work on this too much. I did a bit of <a href="https://arxiv.org/pdf/2406.08929">reading</a>, and then dove right on in because I realised the model could be virtually the same as what I was already working on and thinking about.</p>
<p>I trained two different models, one for the digit dataset and one for the fashion dataset. The only difference between the two setups was the dataset being used, I didn't even tweak a hyperparameter. I started on the digits, and when that worked far better than I could have hoped, I decided to just try it on the fashion dataset without changing anything. In the reconstruction and classifying last week it didn't work great. I'll let the pictures speak for themselves this week though.</p>
<p>The following shows the 10 starting noise patterns, and the corresponding digits produced when conditioned on the appropriate label.</p>
<p><img alt="Digits Output" src="../../../images/diffusion_mnist/digits_output.png"></p>
<p>And similarly for the fashion model</p>
<p><img alt="Fashion Output" src="../../../images/diffusion_mnist/fashion_output.png"></p>
<p>One of the few ablations I actually did was to make sure the label actually mattered. I started with the same noise for each image generation, and only varied the labels. It is hard to see, but you can verify that the initial noise is the same for all images in the following output.</p>
<p><img alt="Digits Output same noise" src="../../../images/diffusion_mnist/same_initial_noise_digits.png"></p>
<p>I think these images are all great for the amount of effort I put in.</p>
<h1>Future</h1>
<p>A small thing of note that I initially wasn't doing is the conditioning on the label. I initially started without any conditioning, but then I obviously couldn't control which digit I was generating, which was a bit disappointing. I think it might be interesting to explore creating a secondary dataset where there are images representing 0-99 and see what can be made from that.</p>
<p>I also think CIFAR-10 is an interesting dataset in terms of challenges. I've spent the last few weeks playing around with a single channel image, whereas CIFAR-10 has three colour channels, which I'm sure has challenges I haven't had to solve yet.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="https://nathan-van-maastricht.github.io/Nathan-Van-Maastricht-blog/tag/machine-learning.html">machine learning</a>
      <a href="https://nathan-van-maastricht.github.io/Nathan-Van-Maastricht-blog/tag/mnist.html">mnist</a>
    </p>
  </div>







</article>

<footer>
<p>&copy;  </p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
</p></footer>  </main>

<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Nathan Van Maastricht Blog ",
  "url" : "https://nathan-van-maastricht.github.io/Nathan-Van-Maastricht-blog",
  "image": "",
  "description": ""
}
</script>
</body>
</html>