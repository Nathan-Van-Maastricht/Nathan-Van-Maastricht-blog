
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />

  <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="https://nathan-van-maastricht.github.io/Nathan-Van-Maastricht-blog/theme/stylesheet/style.min.css">


    <link id="pygments-light-theme" rel="stylesheet" type="text/css"
          href="https://nathan-van-maastricht.github.io/Nathan-Van-Maastricht-blog/theme/pygments/github.min.css">



  <link rel="stylesheet" type="text/css" href="https://nathan-van-maastricht.github.io/Nathan-Van-Maastricht-blog/theme/font-awesome/css/fontawesome.css">
  <link rel="stylesheet" type="text/css" href="https://nathan-van-maastricht.github.io/Nathan-Van-Maastricht-blog/theme/font-awesome/css/brands.css">
  <link rel="stylesheet" type="text/css" href="https://nathan-van-maastricht.github.io/Nathan-Van-Maastricht-blog/theme/font-awesome/css/solid.css">






<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-S4XSZFZ0ZK"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-S4XSZFZ0ZK');
</script>






 

<meta name="author" content="Nathan Van Maastricht" />
<meta name="description" content="The Problem: TSP The Traveling Salesperson Problem (TSP) is one of the most famous combinatorial optimisation problems. It is incredibly easy to state: Given a list of cities and the distances between each pair, find the shortest route that visits each city exactly once and then returns to the first …" />
<meta name="keywords" content="machine learning, optimisation, reinforcement learning">


  <meta property="og:site_name" content="Nathan Van Maastricht Blog"/>
  <meta property="og:title" content="Reinforcement Learning for Combinatorial Optimisation"/>
  <meta property="og:description" content="The Problem: TSP The Traveling Salesperson Problem (TSP) is one of the most famous combinatorial optimisation problems. It is incredibly easy to state: Given a list of cities and the distances between each pair, find the shortest route that visits each city exactly once and then returns to the first …"/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:url" content="https://nathan-van-maastricht.github.io/Nathan-Van-Maastricht-blog/articles/2025/11/reinforcement-learning-for-combinatorial-optimisation.html"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2025-11-02 01:00:00+10:30"/>
  <meta property="article:modified_time" content=""/>
  <meta property="article:author" content="https://nathan-van-maastricht.github.io/Nathan-Van-Maastricht-blog/author/nathan-van-maastricht.html">
  <meta property="article:section" content="Machine Learning,"/>
  <meta property="article:tag" content="machine learning"/>
  <meta property="article:tag" content="optimisation"/>
  <meta property="article:tag" content="reinforcement learning"/>
  <meta property="og:image" content="">

  <title>Nathan Van Maastricht Blog &ndash; Reinforcement Learning for Combinatorial Optimisation</title>


</head>
<body class="light-theme">

<aside>
  <div>
    <a href="https://nathan-van-maastricht.github.io/Nathan-Van-Maastricht-blog/">
      <img src="https://nathan-van-maastricht.github.io/Nathan-Van-Maastricht-blog/theme/img/profile.png" alt="" title="">
    </a>

    <h1>
      <a href="https://nathan-van-maastricht.github.io/Nathan-Van-Maastricht-blog/"></a>
    </h1>



    <nav>
      <ul class="list">


            <li>
              <a target="_self"
                 href="https://nathan-van-maastricht.github.io/Nathan-Van-Maastricht-blog/pages/about.html#about">
                About
              </a>
            </li>

          <li>
            <a target="_self" href="https://www.linkedin.com/in/nathan-van-maastricht-72a177324/" >LinkedIn</a>
          </li>
          <li>
            <a target="_self" href="https://github.com/Nathan-Van-Maastricht" >Github</a>
          </li>
      </ul>
    </nav>

  </div>

</aside>
  <main>


<article class="single">
  <header>
      
    <h1 id="reinforcement-learning-for-combinatorial-optimisation">Reinforcement Learning for Combinatorial Optimisation</h1>
    <p>
      Posted on Sun 02 November 2025 in <a href="https://nathan-van-maastricht.github.io/Nathan-Van-Maastricht-blog/category/machine-learning.html">Machine Learning,</a>

    </p>
    <p>
      Read time: 14 minutes
    </p>
  </header>


  <div>
    <h1>The Problem: TSP</h1>
<p>The Traveling Salesperson Problem (TSP) is one of the most famous combinatorial optimisation problems. It is incredibly easy to state: Given a list of cities and the distances between each pair, find the shortest route that visits each city exactly once and then returns to the first city.</p>
<p>Despite being easy to state, the TSP is NP-hard. The number of possible tours grows factorially, even with just 20 cities, the number of tours that are possible is <span class="math">\(2.4\times10^{18}\)</span>. This explosion in the number of possible routes is exactly why this problem is interesting, and hard.</p>
<p>This post is going to talk about various approaches to picking a good, or even the best tour from the <span class="math">\(n!\)</span> possible tours, with a light introduction to some, and then a reinforcement learning approach that I have implemented and will emperically demonstrate the quality of.</p>
<h1>Classical Approaches</h1>
<p>Classical methods for tackling the TSP fall into three main categories, each with different trade offs between solution quality and computational efficiency.</p>
<h2>Linear Programming and Exact Methods</h2>
<p>Exact algorithms, often based on solving constrained Integer Linear Programs, guarantee the globally optimal solution. One of the primary formulations is the Dantzig-Fulkerson-Johnson model, which uses binary decision variables, <span class="math">\(x_{ij}\in\{0,1\}\)</span>, where <span class="math">\(x_{ij}=1\)</span> if the edge from city <span class="math">\(i\)</span> to city <span class="math">\(j\)</span> is part of the tour, and <span class="math">\(0\)</span> otherwise.</p>
<p>The objective is to minimise the total tour length </p>
<div class="math">$$\min\sum_{i\neq j}c_{ij}x_{ij}$$</div>
<p> where <span class="math">\(c_{ij}\)</span> is the cost of going from city <span class="math">\(i\)</span> to city <span class="math">\(j\)</span>.</p>
<p>This objective is subject to three constraints, in two classes:
1. Depature/Arrival Constraints: Each city must be entered and exited exactly once:
</p>
<div class="math">$$\sum_{j=1}^{N}x_{ij}=1 \forall i=1,\ldots,N\\\sum_{i=1}^{N}x_{ij}=1 \forall j=1,\ldots,N$$</div>
<p>
2. Subtour Elimination Constraints: This constraint ensures there is only a single cycle, rather than multiple, disjoint cycles. </p>
<div class="math">$$\sum_{i\in S}\sum_{j\in S}x_{ij}\leq|S|-1 \forall S\subset V, 2\leq |S|\leq N-1.$$</div>
<p> Since the number of possible subtours is exponential, these constraints are typically added dynamically.</p>
<p>While having fantastic guarantees, methods of this kind are horribly slow, becoming computationally infeasible for even moderately sized instances.</p>
<h3>Advantages</h3>
<ul>
<li>Guaranteed optimality</li>
<li>Based on solid, well-defined mathematics</li>
</ul>
<h3>Disadvantages</h3>
<ul>
<li>Computationally intractable for even moderate sized instances</li>
</ul>
<h2>Heuristics</h2>
<p>Heuristics are typically polynomial time algorithms that aim to build a high quality solution, and don't offer optimality guarantees, although some do give bounds on the optimality gap.</p>
<ul>
<li>
<p>Constructive Heuristics: These are often very fast, but often produce poor results. An example is the nearest neighbour algorithm. It works by randomly picking a starting vertex, and then the next vertex in the sequence is just the nearest vertex that hasn't already been visited.</p>
</li>
<li>
<p>Local Search Heuristics: Theses take some initial tour, such as what is generated by a constructive heuristic, and improve the tour. The search procedure guarantees the tour cost will not increase in a single pass, but it does not guarantee stopping at the global optimal solution, but rather a local optimum.</p>
<ul>
<li>2-opt: The tour is improved by selecting two, non-adjacent edges <span class="math">\((i,j)\)</span> and <span class="math">\((m,n)\)</span>, and replacing them with a different pair of edges, typically <span class="math">\((i, m)\)</span> and <span class="math">\((j, n)\)</span>, provided this decreases the total tour length. 2-opt runs in <span class="math">\(O(N^2)\)</span> per pass. There are different strategies to picking when to stop a pass. One approach might be to stop as soon as an improving move is found. Another approach is to find all improving moves, and then commit to the best one.</li>
<li>3-opt: This selects three edges and considers all seven of the non-trivial ways to reconnect the three resulting segments. This is much better at escaping local optima than 2-opt, but it does run in <span class="math">\(O(N^3)\)</span> time.</li>
<li>Lin-Kernighan: This is an incredibly effect k-opt heuristic that is considered to be the best practical algorithm for the TSP. It performs a sequence of swpas that may temporarily increase the cost, allowing it to escape local minima. A very large drawback to it is the complexity required to implement it efficiently.</li>
</ul>
</li>
</ul>
<h3>Advantages</h3>
<ul>
<li>Speed: Heuristics are typically quite cheap to run, especially the more simple ones like 2-opt and the constructive heuristics.</li>
<li>Simplicity: A lot of heuristics are simple to understand and implement</li>
</ul>
<h3>Disadvantages</h3>
<ul>
<li>No Global Guarantees: Local optima are guaranteed, but not the global optima</li>
<li>Solution Dependence: Local searches often depend on the initial tour provided.</li>
</ul>
<h2>Metaheuristics</h2>
<p>Metaheuristics employ high level strategies to explore solution spaces more effectively than a simple local search, aiming for better solutions, but again without optimality guarantees. They are designed to escaple local optima, which is a major limitation of the pure k-opt heuristics, by occasionally accepting worse moves. There are a number of powerful metahuristics, such as Tabu Search, Genetic Algorithms, and Simulated Annealing, which I will go into more detail on.</p>
<h3>Simulated Annealing (SA)</h3>
<p>Simulated annealing is traditionall a probabilitic metaheuristic inspired by the metallurgical process of annealing, where a material is heated and cooled slowly to increase the crystal size and reduce defects. As it applies to optimisation, the "temperature" controls the probability of accepting a move that increases the tour cost.</p>
<p>At a high temperature (<span class="math">\(T\)</span>), the algorithm is extremely explorative, and frequently accepts worse solutions, allowing the algorithm to move across the search space. As the temperature decreases according to a cooling schedule (such as exponential or linear decay), the probability of moving to a worse state drops, and the algorithm will resemble a local search.</p>
<p>The probability of accepting a worse move depends on what is called the energy (<span class="math">\(E\)</span>) of the system, and is typically the function that is being optimised, in the case of the TSP, the tour length. The acceptance probability is defined by the Metropolis Criterion </p>
<div class="math">$$P(accept)=e^{-\Delta E/T}.$$</div>
<p> This controlled acceptance of bad moves makes SA an effective, general purpose method for finding high quality solutions to many combinatorial problems, and is particularly good for the TSP. It's performce is quite sensitive to the chosen cooling schedule though, so it does require some hyperparameter tuning for each application.</p>
<p>It is also worth noting that there is a deterministic version of SA that is equivalent, and in my experience, is typically what is actually implemented, as it is both easier to implement, and easier to reason about in debugging stages.</p>
<h3>Advantages</h3>
<ul>
<li>High Quality: Generally finds high quality solutions, typically close to optimal</li>
<li>Exploration: Able to reliably escape local optima early in the process</li>
<li>Robust: Less sensitive to initial solutions</li>
</ul>
<h3>Disadvantages</h3>
<ul>
<li>Tuning: Performance is sensitive to hyperparameter tuning</li>
<li>Speed: Slower than simple heuristics</li>
</ul>
<h1>Machine Learning Approaches</h1>
<p>Machine learning gives new approaches for solving complex combinatorial problems. Instead of hard coding an optimisation strategy, a model learns the strategy from either data or experience.</p>
<p>The rest of this blog post is going to focus on my personal approaches to generating tours for the TSP problem, combining ideas from machine learning and heuristics. As I've worked on models and they have improved, metaheuristics have proved to be overkill. I have no practical experience with solving the TSP with a supervised learning approach, but have included a section for completeness. I have focused purely on reinforcement learning approaches.</p>
<h2>Supervised Learning</h2>
<p>In a supervised approach, a model is trained to reproduce the output of an optimal, or near optimal solver such as Lin-Kernighan. This is normally done using cross entropy loss to match the probability distribution of the expert's next edge to add to a tour.</p>
<p>While conceptually relatively simple, this method suffers from some major technical shortcomings that make it inferior to an RL approach for combinatorial optimisation.</p>
<ol>
<li>Poor Generalisation and Scalability: The trained input-output mapping is heavily dependent on the specific number of vertices in the training data. The model learns fixed patterns that struggle to generalise to problem instances with a larger, or even slightly different number of verties than those through the training set</li>
<li>Exposure Bias and Compounding Errors: The model is exclusively trained on ground truth, optimal sequences generated by the expert. However, during inference, if the model makes a suboptimal move early on, the subsequent input context immediately becomes out of distribution. Because the model was never trained on these states, the performance will rapidly degrade, leading to errors that compound more and more, giving a terrible quality for the completed tour.</li>
</ol>
<p>It's mostly for these two reason that the consensus in the field is that RL outperforms supervised learning for the TSP and similar combinatorial optimsation problems, particularly for generalisation.</p>
<h3>Advantages</h3>
<ul>
<li>Training Speed: Quick convergence and simple implementation using standard cross entropy loss</li>
<li>Data Efficiency: Effective when a vast amount of high-quality expert data is available for a single problem size</li>
</ul>
<h3>Disadvantages</h3>
<ul>
<li>Exposure Bias: Prone to compounding errors during inference when it deviates from the expert's path</li>
<li>Poor Generalisation: Fails to generalise to instances with a different number of vertices</li>
<li>Mimicry: Cannot generate solutions better than the expert it was trained on</li>
</ul>
<h2>Reinforcement Learning (RL)</h2>
<p>The RL approach frames the TSP as a sequential decision making process. The model acts as an agent that selects the next city at each step to construct the final tour. The following define the RL setup:</p>
<ul>
<li>Agent: A pointer network is what I have used as it can process a variable length sequence of cities and select one of them as the next step</li>
<li>Environment: The current state of the tour</li>
<li>Reward: The negative of the final tour length, where we are maximising the reward</li>
</ul>
<h3>Pointer Network Architecture</h3>
<p>The pointer network is an encoder-decoder with attention. It is designed for solving sequence-to-sequence problems, where the outpu sequence is a set of indices from the input sequence.</p>
<h4>Encoder</h4>
<p>The model embeds the raw <span class="math">\(N\)</span> vertex coordinates <span class="math">\(x_i\in\mathbb{R}^2\)</span> into a high dimensional space. This initial embedding is then fed through several layers of a standard transformer encoder.</p>
<p>The core of the encoder is a multi-head attention mechanism, which allows the model to understand the relationship between all cities simultaneously. This process generates a contextual embedding for every vertex <span class="math">\(i\)</span>, encoding its importance and relationship to all other vertices in the instance.</p>
<h4>Decoder</h4>
<p>The decoder is an LSTM-based recurrent unit, which is responsible for generating the tour sequentially. At each decoding step, <span class="math">\(t\)</span>, the hidden state <span class="math">\(h_t\)</span> is updated, with the input being the embedding of the previously selected vertex, <span class="math">\(e_prev\)</span>. The hidden state <span class="math">\(h_t\)</span> forms the query vector for the attention mechanism.</p>
<h4>Attention Mechanism</h4>
<p>The decoder's hidden state <span class="math">\(h_t\)</span> queries all encoded city embeddings, <span class="math">\(e_i\)</span>. This generates the unnormalised logit scores, <span class="math">\(u_t^i\)</span>, that indicate the preference for selecting city <span class="math">\(i\)</span> as the next action <span class="math">\(a_t\)</span>. A tanh-based attention mechanism is used: </p>
<div class="math">$$u_t^i=\mathbf{v^\top}\tanh\left(\mathbf{W_{\text{ref}}}e_i+\mathbf{W_{\text{query}}}h_t\right)$$</div>
<p> where <span class="math">\(\mathbf{v}\)</span>, <span class="math">\(\mathbf{W_{\text{ref}}}\)</span>, and <span class="math">\(\mathbf{W_{\text{query}}}\)</span> are learnable parameters.</p>
<h4>Constraint Masking</h4>
<p>While this is not required, it drastically speeds up training, and makes the reward function simple, rather than requiring a penalty for an incomplete tour. The raw logits, <span class="math">\(u_t\)</span> get filtered to ensure that only unvisited vertices can be selected. The mask is applied before the final softmax.</p>
<h4>Policy Gradient Training using REINFORCE</h4>
<p>The agent is trained using the REINFORCE algorithm, which directly optimises the expected reward. The objective is to maximise </p>
<div class="math">$$J(\theta)=\mathbb{E}_{\pi(\tau|\theta)}[R(\tau)]$$</div>
<p> where <span class="math">\(\tau\)</span> is the tour sequence generated by the policy <span class="math">\(\pi\)</span> and <span class="math">\(R(\tau)=-L(\tau)\)</span> is the negative tour length.</p>
<p>The gradient is approximated using Monte Carlo sampling: </p>
<div class="math">$$\nabla J(\theta)\approx\frac{1}{B}\sum_{b=1}^B\nabla\log\pi(\tau_b|\theta)(R(\tau_b)-B_t)$$</div>
<h4>EMA Baseline (<span class="math">\(B_t\)</span>)</h4>
<p>The term <span class="math">\(B_t\)</span> is a learned baseline, and in this case is just an exponential moving average of the rewards, which is used to reduce the high variance of the gradient estimates, without introducing bias. The advantage term, <span class="math">\(R(\tau_b)-B_t\)</span>, only updates the policy for tours that perform better than the historical average, leading to more stable training. The baseline is defined by </p>
<div class="math">$$B_t=(1-\gamma)R_{\text{mean}}+\gamma B_{t-1}$$</div>
<p> when <span class="math">\(t&gt;0\)</span>, and just <span class="math">\(B_t=R_{\text{mean}}\)</span> for the first iteration.</p>
<h4>Hyperparameter Discussion</h4>
<p>A batch size of 64 was used, and 10000 epochs were ran. The learning rate was fixed at 0.0005, and the model was trained on 100 vertex instances only.</p>
<p>Some simple improvements to this setup would involve decaying the learning rate over time, training on a variable instance size, and training longer.</p>
<p>In a different, but related model that also solves the TSP, I have introduced a second model to the training process that generates instances as well. Due to time constraints, I have not done that here, but it proved effective, especially at generalising outside of the standard way to generate instances, which is to just sample uniformly in the unit square until the desired number of vertices are obtained. This second model does pose challenges though with one, or both models, degrading into a collapsed state.</p>
<h3>Inference</h3>
<p>Instead of sampling over the logits, an argmax is used at inference time. I also combine the pointer network, and a naive 2-opt was performed to fix any local issues, implemented in python.</p>
<h1>Results</h1>
<p>For a 100 vertex instance we obtain the following, in roughly 1 second. About half the time was spent in the pointer network computation, and half in the 2-opt.</p>
<p><img alt="100 vertices" src="../../../images/travelling_salesman_problem/improved_100.png"></p>
<p>For a 250 vertex instance, we obtain the following, in roughly 6 seconds. About 0.7 seconds was spent in the pointer network computation.</p>
<p><img alt="250 vertices" src="../../../images/travelling_salesman_problem/improved_250.png"></p>
<p>And for a 500 vertex instance, we obtain the following, in roughly 15 seconds. About 0.9 seconds was spent in the pointer network computation.</p>
<p><img alt="500 vertices" src="../../../images/travelling_salesman_problem/improved_500.png"></p>
<p>One obvious way to improve the runtime performce is to implement even the naive 2-opt in a lower level language. The model does not readily export to a format that can be ran easily in a language like Rust though, so I have forgone that for now. I could also use a better 2-opt algorithm, rather than the naive implementation I have gone for.</p>
<p>As for the actual solutions that are generated, they are of high quality, especially given the time. I have not benchmarked them against other methods, such as simulated annealing or more simple constructive methods. But I have had experience benchmarking the other model against those approaches, and my gut tells me these solutions are of similar quality to the previous model I've developed. I do endevour to do this benchmarking at a later date, and will likely write a detailed post about that benchmarking too.</p>
<h1>Future Work</h1>
<p>While REINFORCE with an EMA baseline provides a good foundation, there are more robust ways to train models like this.</p>
<h2>Advanced Policy Optimisation</h2>
<p>A limitation of REINFORCE is its high gradient variance. Replacing it with an off-policy RL algorithm would make training more stable, and likely increase convergence speed. There are two primary avenues</p>
<ol>
<li>Actor-Critic: Directly train a separate critic network to learn the state value, rather than using an EMA as a baseline, which should dramatically reduce the variance in the advantage estimate.</li>
<li>Proximal Policy Optimisation: Use a clipped objective function to constrain policy updates, which will prevent large and often destructive steps allowing for better data reuse.</li>
</ol>
<h2>Integration of Heuristics</h2>
<p>The current approach uses 2-opt only after the model generates a full tour. Two possible ways to incorporate learning into the heuristics are:</p>
<ol>
<li>Neural Guidance: Swap to a 3-opt approach, and use the model's attention scores and predicted probabilities to bias the 3-opt heuristic to prioritise swaps</li>
<li>Graph Coarsening: For large instances, allow a model to learn to identify local clusters of vertices first, and solve those clusters, before solving the higher level TSP</li>
</ol>
<h2>Hierarchical Attention</h2>
<p>To approach much larger instances, with thousands of vertices rather than hundreds, and to overcome the <span class="math">\(O(N^2)\)</span> complexity of the attention mechanism currently a hierarchical attention mechanism can be used by decomposing the attention processes into local and global attention, which would allow for reducing the complexity from quadratic to log-linear.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="https://nathan-van-maastricht.github.io/Nathan-Van-Maastricht-blog/tag/machine-learning.html">machine learning</a>
      <a href="https://nathan-van-maastricht.github.io/Nathan-Van-Maastricht-blog/tag/optimisation.html">optimisation</a>
      <a href="https://nathan-van-maastricht.github.io/Nathan-Van-Maastricht-blog/tag/reinforcement-learning.html">reinforcement learning</a>
    </p>
  </div>







</article>

<footer>
<p>&copy;  </p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
</p></footer>  </main>

<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Nathan Van Maastricht Blog ",
  "url" : "https://nathan-van-maastricht.github.io/Nathan-Van-Maastricht-blog",
  "image": "",
  "description": ""
}
</script>
</body>
</html>